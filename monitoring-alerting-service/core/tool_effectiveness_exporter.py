"""
Tool Effectiveness Metrics Exporter

This module exports tool effectiveness metrics to Prometheus for monitoring and alerting.
It periodically fetches metrics from the analysis engine API and converts them
to Prometheus metrics for visualization in Grafana.
"""
import time
import logging
import requests
from typing import Dict, List, Any, Optional
import os
import json
from datetime import datetime, timedelta
from prometheus_client import start_http_server, Gauge, Counter, Summary
logging.basicConfig(level=logging.INFO, format=
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
ANALYSIS_ENGINE_BASE_URL = os.environ.get('ANALYSIS_ENGINE_BASE_URL',
    'http://localhost:8000')
METRICS_PORT = int(os.environ.get('METRICS_PORT', 8001))
METRICS_UPDATE_INTERVAL = int(os.environ.get('METRICS_UPDATE_INTERVAL', 60))
TOOL_WIN_RATE = Gauge('tool_effectiveness_win_rate',
    'Win rate percentage for trading tools', ['tool_id', 'timeframe',
    'market_regime'])
TOOL_PROFIT_FACTOR = Gauge('tool_effectiveness_profit_factor',
    'Profit factor (sum of profits / sum of losses)', ['tool_id',
    'timeframe', 'market_regime'])
TOOL_EXPECTED_PAYOFF = Gauge('tool_effectiveness_expected_payoff',
    'Expected payoff per trade', ['tool_id', 'timeframe', 'market_regime'])
TOOL_SIGNAL_COUNT = Gauge('tool_effectiveness_signal_count',
    'Number of signals generated by the tool', ['tool_id'])
TOOL_OUTCOME_COUNT = Gauge('tool_effectiveness_outcome_count',
    'Number of outcomes recorded for the tool', ['tool_id', 'outcome'])
SIGNAL_QUALITY_SCORE = Gauge('signal_quality_overall_score',
    'Overall quality score for signals', ['tool_id'])
SIGNAL_QUALITY_BY_REGIME = Gauge('signal_quality_by_regime',
    'Signal quality score by market regime', ['tool_id', 'market_regime'])
QUALITY_TREND = Gauge('tool_effectiveness_quality_trend',
    'Trend in signal quality over time', ['tool_id', 'timeframe'])
SUCCESS_TREND = Gauge('tool_effectiveness_success_trend',
    'Trend in success rate over time', ['tool_id', 'timeframe'])
REQUEST_LATENCY = Summary('http_request_duration_seconds',
    'HTTP request latency in seconds', ['endpoint'])
REQUEST_FAILURES = Counter('http_request_failures_total',
    'Total count of HTTP request failures', ['endpoint'])


from core.exceptions_bridge_1 import (
    with_exception_handling,
    async_with_exception_handling,
    ForexTradingPlatformError,
    ServiceError,
    DataError,
    ValidationError
)

class ToolEffectivenessExporter:
    """
    Prometheus exporter for tool effectiveness metrics.
    Collects metrics from the analysis-engine-service API and
    exposes them for Prometheus.
    """

    def __init__(self, base_url: str, update_interval: int):
        """
        Initialize the exporter with configuration

        Args:
            base_url: Base URL of the analysis engine API
            update_interval: How often to update metrics (in seconds)
        """
        self.base_url = base_url
        self.update_interval = update_interval
        self.session = requests.Session()
        self.logger = logging.getLogger(__name__)
        self.logger.info(
            f'Initialized Tool Effectiveness Exporter with base URL: {base_url}'
            )

    @with_exception_handling
    def start(self):
        """
        Start the exporter server and begin periodic metric collection
        """
        start_http_server(METRICS_PORT)
        self.logger.info(f'Started metrics server on port {METRICS_PORT}')
        while True:
            try:
                self.collect_metrics()
            except Exception as e:
                self.logger.error(f'Error collecting metrics: {str(e)}')
            time.sleep(self.update_interval)

    def collect_metrics(self):
        """
        Collect and update all metrics
        """
        self.logger.info('Collecting metrics...')
        start_time = time.time()
        self.collect_effectiveness_metrics()
        self.collect_signal_quality_metrics()
        self.collect_trend_metrics()
        self.collect_market_regime_metrics()
        end_time = time.time()
        self.logger.info(
            f'Metrics collection completed in {end_time - start_time:.2f} seconds'
            )

    @with_exception_handling
    def collect_effectiveness_metrics(self):
        """
        Collect effectiveness metrics from the API
        """
        try:
            url = f'{self.base_url}/api/v1/tool-effectiveness/metrics'
            request_time = REQUEST_LATENCY.labels(endpoint='metrics')
            with request_time.time():
                response = self.session.get(url)
            if response.status_code != 200:
                self.logger.error(
                    f'Failed to get effectiveness metrics: {response.status_code} - {response.text}'
                    )
                REQUEST_FAILURES.labels(endpoint='metrics').inc()
                return
            metrics_data = response.json()
            for tool_data in metrics_data:
                tool_id = tool_data.get('tool_id', 'unknown')
                TOOL_SIGNAL_COUNT.labels(tool_id=tool_id).set(tool_data.get
                    ('signal_count', 0))
                success_count = int(tool_data.get('success_rate', 0) *
                    tool_data.get('signal_count', 0) / 100)
                failure_count = tool_data.get('signal_count', 0
                    ) - success_count
                TOOL_OUTCOME_COUNT.labels(tool_id=tool_id, outcome='success'
                    ).set(success_count)
                TOOL_OUTCOME_COUNT.labels(tool_id=tool_id, outcome='failure'
                    ).set(failure_count)
                for metric in tool_data.get('metrics', []):
                    name = metric.get('name', '').lower()
                    value = metric.get('value', 0)
                    if 'win rate' in name:
                        TOOL_WIN_RATE.labels(tool_id=tool_id, timeframe=
                            'all', market_regime='all').set(value)
                    elif 'profit factor' in name:
                        TOOL_PROFIT_FACTOR.labels(tool_id=tool_id,
                            timeframe='all', market_regime='all').set(value)
                    elif 'expected payoff' in name:
                        TOOL_EXPECTED_PAYOFF.labels(tool_id=tool_id,
                            timeframe='all', market_regime='all').set(value)
                    elif 'reliability in' in name.lower():
                        regime = name.lower().replace('reliability in ', ''
                            ).replace(' market', '')
                        TOOL_WIN_RATE.labels(tool_id=tool_id, timeframe=
                            'all', market_regime=regime).set(value)
            self.logger.info(
                f'Updated effectiveness metrics for {len(metrics_data)} tools')
        except Exception as e:
            self.logger.error(
                f'Error collecting effectiveness metrics: {str(e)}')
            REQUEST_FAILURES.labels(endpoint='metrics').inc()

    @with_exception_handling
    def collect_signal_quality_metrics(self):
        """
        Collect signal quality metrics from the API
        """
        try:
            url = f'{self.base_url}/api/v1/tool-effectiveness/dashboard-data'
            request_time = REQUEST_LATENCY.labels(endpoint='dashboard-data')
            with request_time.time():
                response = self.session.get(url)
            if response.status_code != 200:
                self.logger.error(
                    f'Failed to get dashboard data: {response.status_code} - {response.text}'
                    )
                REQUEST_FAILURES.labels(endpoint='dashboard-data').inc()
                return
            dashboard_data = response.json()
            tool_ids = dashboard_data.get('filters', {}).get('tools', [])
            for tool_id in tool_ids:
                url = (
                    f'{self.base_url}/api/v1/signal-quality/quality-analysis?tool_id={tool_id}'
                    )
                request_time = REQUEST_LATENCY.labels(endpoint=
                    'quality-analysis')
                try:
                    with request_time.time():
                        response = self.session.get(url)
                    if response.status_code != 200:
                        self.logger.warning(
                            f'Failed to get quality analysis for {tool_id}: {response.status_code}'
                            )
                        continue
                    analysis = response.json()
                    SIGNAL_QUALITY_SCORE.labels(tool_id=tool_id).set(analysis
                        .get('average_quality', 0))
                    for bracket in analysis.get('quality_brackets', []):
                        regime_label = (
                            f"q{int(bracket['quality_range'][0] * 10)}_to_q{int(bracket['quality_range'][1] * 10)}"
                            )
                        SIGNAL_QUALITY_BY_REGIME.labels(tool_id=tool_id,
                            market_regime=regime_label).set(bracket.get(
                            'success_rate', 0))
                except Exception as e:
                    self.logger.error(
                        f'Error collecting quality analysis for {tool_id}: {str(e)}'
                        )
                    REQUEST_FAILURES.labels(endpoint='quality-analysis').inc()
            self.logger.info(
                f'Updated quality metrics for {len(tool_ids)} tools')
        except Exception as e:
            self.logger.error(
                f'Error collecting signal quality metrics: {str(e)}')
            REQUEST_FAILURES.labels(endpoint='quality-metrics').inc()

    @with_exception_handling
    def collect_trend_metrics(self):
        """
        Collect trend metrics from the API
        """
        try:
            url = f'{self.base_url}/api/v1/tool-effectiveness/dashboard-data'
            request_time = REQUEST_LATENCY.labels(endpoint='dashboard-data')
            with request_time.time():
                response = self.session.get(url)
            if response.status_code != 200:
                self.logger.error(
                    f'Failed to get dashboard data: {response.status_code} - {response.text}'
                    )
                REQUEST_FAILURES.labels(endpoint='dashboard-data').inc()
                return
            dashboard_data = response.json()
            tool_ids = dashboard_data.get('filters', {}).get('tools', [])
            for tool_id in tool_ids:
                url = (
                    f'{self.base_url}/api/v1/signal-quality/quality-trends?tool_id={tool_id}'
                    )
                request_time = REQUEST_LATENCY.labels(endpoint='quality-trends'
                    )
                try:
                    with request_time.time():
                        response = self.session.get(url)
                    if response.status_code != 200:
                        self.logger.warning(
                            f'Failed to get quality trends for {tool_id}: {response.status_code}'
                            )
                        continue
                    trends = response.json()
                    QUALITY_TREND.labels(tool_id=tool_id, timeframe='all').set(
                        trends.get('quality_trend', 0))
                    SUCCESS_TREND.labels(tool_id=tool_id, timeframe='all').set(
                        trends.get('success_trend', 0))
                except Exception as e:
                    self.logger.error(
                        f'Error collecting quality trends for {tool_id}: {str(e)}'
                        )
                    REQUEST_FAILURES.labels(endpoint='quality-trends').inc()
            self.logger.info(f'Updated trend metrics for {len(tool_ids)} tools'
                )
        except Exception as e:
            self.logger.error(f'Error collecting trend metrics: {str(e)}')
            REQUEST_FAILURES.labels(endpoint='trend-metrics').inc()

    @with_exception_handling
    def collect_market_regime_metrics(self):
        """
        Collect market regime reliability metrics from the API
        """
        try:
            url = (
                f'{self.base_url}/api/v1/tool-effectiveness/regime-reliability'
                )
            request_time = REQUEST_LATENCY.labels(endpoint='regime-reliability'
                )
            with request_time.time():
                response = self.session.get(url)
            if response.status_code != 200:
                self.logger.error(
                    f'Failed to get market regime metrics: {response.status_code} - {response.text}'
                    )
                REQUEST_FAILURES.labels(endpoint='regime-reliability').inc()
                return
            regime_data = response.json()
            for tool_data in regime_data:
                tool_id = tool_data.get('tool_id', 'unknown')
                for regime, metrics in tool_data.get('regime_metrics', {}
                    ).items():
                    win_rate = metrics.get('win_rate', 0)
                    profit_factor = metrics.get('profit_factor', 1.0)
                    expected_payoff = metrics.get('expected_payoff', 0)
                    TOOL_WIN_RATE.labels(tool_id=tool_id, timeframe='all',
                        market_regime=regime).set(win_rate)
                    TOOL_PROFIT_FACTOR.labels(tool_id=tool_id, timeframe=
                        'all', market_regime=regime).set(profit_factor)
                    TOOL_EXPECTED_PAYOFF.labels(tool_id=tool_id, timeframe=
                        'all', market_regime=regime).set(expected_payoff)
                    if 'quality_score' in metrics:
                        SIGNAL_QUALITY_BY_REGIME.labels(tool_id=tool_id,
                            market_regime=regime).set(metrics.get(
                            'quality_score', 0))
            self.logger.info(
                f'Updated market regime metrics for {len(regime_data)} tools')
        except Exception as e:
            self.logger.error(
                f'Error collecting market regime metrics: {str(e)}')
            REQUEST_FAILURES.labels(endpoint='regime-reliability').inc()


if __name__ == '__main__':
    logger.info('Starting Tool Effectiveness Metrics Exporter')
    exporter = ToolEffectivenessExporter(base_url=ANALYSIS_ENGINE_BASE_URL,
        update_interval=METRICS_UPDATE_INTERVAL)
    exporter.start()
