\"\"\"\nCausal Algorithm Validation Module\n\nProvides tools for validating causal inference algorithms, primarily using synthetic data\nwith known ground truth causal structures.\n\"\"\"\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Any, List, Optional, Tuple\n\n# Potential imports (add specific libraries as needed)\n# import networkx as nx\n# from sklearn.metrics import precision_recall_fscore_support\n\nlogger = logging.getLogger(__name__)\n\nclass SyntheticDataGenerator:\n    \"\"\"\n    Generates synthetic datasets with pre-defined causal structures.\n    Useful for testing and benchmarking causal discovery algorithms.\n    \"\"\"\n    def __init__(self, parameters: Optional[Dict[str, Any]] = None):\n        self.parameters = parameters or {}\n        logger.info(f\"Initializing SyntheticDataGenerator with parameters: {self.parameters}\")\n\n    def generate_linear_gaussian(self, structure: Dict[str, List[str]], num_samples: int, noise_std: float = 0.5, seed: Optional[int] = None) -> Tuple[pd.DataFrame, Any]:\n        \"\"\"\n        Generates data from a linear Structural Equation Model (SEM) with Gaussian noise.\n\n        Args:\n            structure: Dictionary defining the causal graph (e.g., {\'X\': [], \'Y\': [\'X\'], \'Z\': [\'Y\']}).\n                       Keys are nodes, values are lists of direct parents.\n            num_samples: Number of data points to generate.\n            noise_std: Standard deviation of the Gaussian noise added to each variable.\n            seed: Random seed for reproducibility.\n\n        Returns:\n            Tuple containing:\n                - DataFrame with the generated synthetic data.\n                - The ground truth causal graph (e.g., networkx.DiGraph).\n        \"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        logger.info(f\"Generating {num_samples} samples from linear Gaussian SEM.\")\n        nodes = list(structure.keys())\n        data = pd.DataFrame(columns=nodes, index=range(num_samples))\n\n        # Determine topological order (simple version for DAGs)\n        ordered_nodes = []\n        nodes_to_process = list(nodes)\n        while nodes_to_process:\n            found_node = False\n            for node in list(nodes_to_process): # Iterate over a copy\n                parents = structure.get(node, [])\n                if all(parent in ordered_nodes for parent in parents):\n                    ordered_nodes.append(node)\n                    nodes_to_process.remove(node)\n                    found_node = True\n            if not found_node and nodes_to_process: # Cycle detection or error\n                 logger.error(\"Could not determine topological order. Check graph structure for cycles.\")\n                 return pd.DataFrame(), None # Return empty df and None graph\n\n        # Generate data according to topological order\n        for node in ordered_nodes:\n            parents = structure.get(node, [])\n            if not parents:\n                # Root node\n                data[node] = np.random.normal(0, 1, num_samples) # Base noise\n            else:\n                # Generate based on parents\n                parent_data = data[parents]\n                # Assign random coefficients (e.g., between 0.5 and 1.5)\n                coeffs = np.random.uniform(0.5, 1.5, size=len(parents))\n                linear_combination = np.dot(parent_data, coeffs)\n                data[node] = linear_combination + np.random.normal(0, noise_std, num_samples)\n\n        # Create ground truth graph (e.g., using networkx)\n        try:\n            import networkx as nx\n            ground_truth_graph = nx.DiGraph()\n            for node, parents in structure.items():\n                for parent in parents:\n                    ground_truth_graph.add_edge(parent, node)\n        except ImportError:\n            logger.warning(\"networkx not installed. Ground truth graph will be the structure dict.\")\n            ground_truth_graph = structure\n        except Exception as e:\n             logger.error(f\"Error creating ground truth graph: {e}\")\n             ground_truth_graph = None\n\n        logger.info(\"Synthetic data generation complete.\")\n        return data, ground_truth_graph\n\n    def generate_forex_like_ts(self, num_samples: int, num_series: int = 3, lag: int = 1, influence_coeffs: Optional[np.ndarray] = None, noise_std: float = 0.1, seed: Optional[int] = None) -> pd.DataFrame:\n        \"\"\"\n        Generates multiple time series with lagged dependencies, simulating Forex data.\n\n        Args:\n            num_samples: Length of the time series.\n            num_series: Number of time series to generate.\n            lag: The lag for dependencies (e.g., lag=1 means X_t depends on Y_{t-1}).\n            influence_coeffs: Optional (num_series x num_series) matrix defining influence strengths.\n                              If None, random coefficients are generated.\n            noise_std: Standard deviation of the noise.\n            seed: Random seed.\n\n        Returns:\n            DataFrame containing the generated time series.\n        \"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        logger.info(f\"Generating {num_series} Forex-like time series of length {num_samples} with lag {lag}.\")\n\n        if influence_coeffs is None:\n            # Generate random influence matrix (ensure diagonal is 0 for self-influence at lag 0)\n            influence_coeffs = np.random.uniform(-0.8, 0.8, size=(num_series, num_series))\n            np.fill_diagonal(influence_coeffs, 0)\n        elif influence_coeffs.shape != (num_series, num_series):\n            logger.error(\"influence_coeffs matrix shape mismatch.\")\n            return pd.DataFrame()\n\n        # Initialize data array (add extra rows for initial lags)\n        data_array = np.zeros((num_samples + lag, num_series))\n        # Start with some random noise\n        data_array[:lag, :] = np.random.normal(0, noise_std * 2, size=(lag, num_series))\n\n        # Generate time series step by step\n        for t in range(lag, num_samples + lag):\n            # Get lagged values (shape: num_series)\n            lagged_values = data_array[t - lag, :]\n            # Calculate influence from other series at lag\n            influence = np.dot(influence_coeffs, lagged_values)\n            # Add noise\n            noise = np.random.normal(0, noise_std, size=num_series)\n            # Update current values\n            data_array[t, :] = influence + noise\n\n        # Create DataFrame\n        columns = [f\'Series_{i}\' for i in range(num_series)]\n        df = pd.DataFrame(data_array[lag:, :], columns=columns)\n        # Optionally add a time index\n        df.index = pd.date_range(start=\'2024-01-01\', periods=num_samples, freq=\'D\')\n\n        logger.info(\"Forex-like time series generation complete.\")\n        return df\n\nclass CausalAlgorithmValidator:\n    \"\"\"\n    Validates causal discovery algorithms against a known ground truth graph.\n    Calculates metrics like Structural Hamming Distance (SHD), Precision, Recall, F1.\n    \"\"\"\n    def __init__(self, parameters: Optional[Dict[str, Any]] = None):\n        self.parameters = parameters or {}\n        logger.info(f\"Initializing CausalAlgorithmValidator with parameters: {self.parameters}\")\n\n    def _graph_to_adj_matrix(self, graph: Any, nodes: List[str]) -> Optional[np.ndarray]:\n        \"\"\"Converts a graph (networkx or dict) to an adjacency matrix.\"\"\"\n        adj_matrix = np.zeros((len(nodes), len(nodes)), dtype=int)\n        node_to_index = {node: i for i, node in enumerate(nodes)}\n\n        try:\n            if hasattr(graph, \'edges\'): # Assuming networkx graph\n                for u, v in graph.edges():\n                    if u in node_to_index and v in node_to_index:\n                        adj_matrix[node_to_index[u], node_to_index[v]] = 1\            elif isinstance(graph, dict): # Assuming structure dict {node: [parents]}\n                 for node, parents in graph.items():\n                     if node in node_to_index:\n                         for parent in parents:\n                             if parent in node_to_index:\n                                 adj_matrix[node_to_index[parent], node_to_index[node]] = 1\n            else:\n                logger.error(\"Unsupported graph type for adjacency matrix conversion.\")\n                return None\n            return adj_matrix\n        except Exception as e:\
            logger.error(f\"Error converting graph to adjacency matrix: {e}\")\
            return None\n\n    def compare_graphs(self, learned_graph: Any, true_graph: Any, nodes: List[str]) -> Dict[str, Any]:\n        \"\"\"\n        Compares a learned causal graph to the ground truth graph.\n\n        Args:\n            learned_graph: The graph learned by the algorithm.\n            true_graph: The ground truth causal graph.\n            nodes: List of node names in the order used for adjacency matrices.\n\n        Returns:\n            Dictionary containing comparison metrics (SHD, Precision, Recall, F1).\n        \"\"\"\n        logger.info(\"Comparing learned graph to true graph.\")\n        metrics = {\n            \"shd\": None, # Structural Hamming Distance\n            \"precision\": None,\n            \"recall\": None,\n            \"f1_score\": None,\n            \"extra_edges\": [],\n            \"missing_edges\": [],\n            \"reversed_edges\": []\n        }\n\n        true_adj = self._graph_to_adj_matrix(true_graph, nodes)\n        learned_adj = self._graph_to_adj_matrix(learned_graph, nodes)\n\n        if true_adj is None or learned_adj is None:\n            logger.error(\"Could not convert graphs to adjacency matrices. Cannot compare.\")\n            return metrics\n\n        if true_adj.shape != learned_adj.shape:\n            logger.error(\"Adjacency matrix shapes do not match. Cannot compare.\")\n            return metrics\n\n        # Calculate differences\n        diff_matrix = learned_adj - true_adj\n        num_nodes = len(nodes)\n\n        extra_edges_count = 0\n        missing_edges_count = 0\n        reversed_edges_count = 0\n        true_positives = 0\n        false_positives = 0\n        false_negatives = 0\n\n        for i in range(num_nodes):\n            for j in range(num_nodes):\n                if i == j: continue\n\n                true_edge = true_adj[i, j] == 1\n                learned_edge = learned_adj[i, j] == 1\n\n                if learned_edge and not true_edge:\n                    # Check if it\'s a reversed edge\n                    if true_adj[j, i] == 1:\n                        # Only count reversal once\n                        if i < j:\n                             reversed_edges_count += 1\n                             metrics[\"reversed_edges\"].append((nodes[j], nodes[i])) # True edge was j->i\n                    else:\n                        extra_edges_count += 1\n                        metrics[\"extra_edges\"].append((nodes[i], nodes[j]))\n                        false_positives += 1\n                elif not learned_edge and true_edge:\n                    # Check if it\'s missing due to reversal counted above\n                    if learned_adj[j, i] != 1:\n                        missing_edges_count += 1\n                        metrics[\"missing_edges\"].append((nodes[i], nodes[j]))\n                        false_negatives += 1\n                elif learned_edge and true_edge:\n                    true_positives += 1\n\n        # Structural Hamming Distance (SHD): extra + missing + reversed\n        metrics[\"shd\"] = extra_edges_count + missing_edges_count + reversed_edges_count\n\n        # Precision, Recall, F1\n        if (true_positives + false_positives) > 0:\n            metrics[\"precision\"] = true_positives / (true_positives + false_positives)\n        else:\n            metrics[\"precision\"] = 0.0\n\n        if (true_positives + false_negatives) > 0:\n            metrics[\"recall\"] = true_positives / (true_positives + false_negatives)\n        else:\n            metrics[\"recall\"] = 0.0\n\n        if (metrics[\"precision\"] + metrics[\"recall\"]) > 0:\n            metrics[\"f1_score\"] = 2 * (metrics[\"precision\"] * metrics[\"recall\"]) / (metrics[\"precision\"] + metrics[\"recall\"])\n        else:\n            metrics[\"f1_score\"] = 0.0\n\n        logger.info(f\"Graph comparison results: {metrics}\")\n        return metrics\n\nclass ForexCausalValidation:\n    \"\"\"\n    Performs validation specific to Forex causal models, like checking stability over time.\n    (Placeholder for more advanced Forex-specific validation methods)\n    \"\"\"\n    def __init__(self, parameters: Optional[Dict[str, Any]] = None):\n        self.parameters = parameters or {}\n        logger.info(f\"Initializing ForexCausalValidation with parameters: {self.parameters}\")\n\n    def check_stability(self, causal_discovery_func: callable, data: pd.DataFrame, time_windows: List[Tuple[int, int]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Checks the stability of discovered causal relationships across different time windows.\n\n        Args:\n            causal_discovery_func: A function that takes a DataFrame and returns a learned graph.\n            data: The full time series DataFrame.\n            time_windows: List of (start_index, end_index) tuples defining time windows.\n\n        Returns:\n            List of comparison results between consecutive windows or against a reference window.\n        \"\"\"\n        logger.info(f\"Checking causal graph stability across {len(time_windows)} windows.\")\n        graphs = []\n        results = []\n\n        # Discover graph for each window\n        for i, (start, end) in enumerate(time_windows):\n            window_data = data.iloc[start:end]\n            if window_data.empty:\n                 logger.warning(f\"Skipping empty time window {i+1} ({start}-{end}).\")\n                 graphs.append(None)\n                 continue\n            try:\n                logger.debug(f\"Running causal discovery on window {i+1} ({start}-{end})...\")\n                learned_graph = causal_discovery_func(window_data)\n                graphs.append(learned_graph)\n            except Exception as e:\n                logger.error(f\"Error during causal discovery for window {i+1}: {e}\")\n                graphs.append(None)\n\n        # Compare graphs (e.g., consecutive windows)\n        validator = CausalAlgorithmValidator()\n        nodes = list(data.columns)\n        for i in range(len(graphs) - 1):\n            g1 = graphs[i]\n            g2 = graphs[i+1]\n            if g1 is not None and g2 is not None:\n                logger.debug(f\"Comparing graph from window {i+1} to window {i+2}.\")\n                comparison = validator.compare_graphs(g2, g1, nodes)\n                results.append({\n                    \"window_1_indices\": time_windows[i],\n                    \"window_2_indices\": time_windows[i+1],\n                    \"comparison_metrics\": comparison\n                })\n            else:\n                 logger.warning(f\"Cannot compare graphs for windows {i+1} and {i+2} due to errors or empty data.\")\n\n        logger.info(\"Stability check complete.\")\n        return results\n\n# Example usage (for testing purposes)\nif __name__ == \'__main__\':\n    print(\"--- Synthetic Data Generator --- \")\n    generator = SyntheticDataGenerator()\n\n    # Linear Gaussian Example\n    structure = {\'A\': [], \'B\': [\'A\'], \'C\': [\'A\', \'B\']}\n    linear_data, true_graph_lg = generator.generate_linear_gaussian(structure, num_samples=500, seed=42)\n    print(f\"Linear Gaussian Data Head:\\n{linear_data.head()}\")\n    if true_graph_lg:\n        try:\n            import networkx as nx\n            print(f\"True Linear Graph Edges: {list(true_graph_lg.edges())}\")\n        except ImportError:\n             print(f\"True Linear Graph Structure: {true_graph_lg}\")\n\n    # Forex-like Time Series Example\n    ts_data = generator.generate_forex_like_ts(num_samples=300, num_series=3, lag=1, seed=43)\n    print(f\"\\nForex-like Time Series Data Head:\\n{ts_data.head()}\")\n\n    print(\"\\n--- Causal Algorithm Validator --- \")\n    validator = CausalAlgorithmValidator()\n\n    # Dummy learned graph (e.g., missing one edge, adding one wrong edge)\n    import networkx as nx\n    learned_graph_lg = nx.DiGraph()\n    learned_graph_lg.add_edges_from([(\'A\', \'B\'), (\'B\', \'C\'), (\'A\', \'C\')]) # Correct edges\n    # learned_graph_lg.add_edge(\'C\', \'A\') # Add wrong edge\n    learned_graph_lg.add_edge(\'B\', \'A\') # Add reversed edge\n\n    if true_graph_lg:\n        nodes_lg = list(structure.keys())\n        metrics = validator.compare_graphs(learned_graph_lg, true_graph_lg, nodes_lg)\n        print(f\"\\nGraph Comparison Metrics (Linear Gaussian):\\n{metrics}\")\n\n    print(\"\\n--- Forex Causal Validation (Stability) --- \")\n    forex_validator = ForexCausalValidation()\n\n    # Dummy causal discovery function (returns a slightly different graph each time)\n    def dummy_discovery(df):\n        g = nx.DiGraph()\n        nodes = list(df.columns)\n        if len(nodes) >= 2:\n            g.add_edge(nodes[0], nodes[1]) # Base edge\n        if len(nodes) >= 3 and np.random.rand() > 0.5:\n             g.add_edge(nodes[1], nodes[2]) # Sometimes add this edge\n        return g\n\n    # Define time windows for stability check\n    window_size = 100\n    step = 50\n    windows = []\n    for i in range(0, len(ts_data) - window_size + 1, step):\n        windows.append((i, i + window_size))\n\n    stability_results = forex_validator.check_stability(dummy_discovery, ts_data, windows)\n    print(f\"\\nStability Check Results ({len(stability_results)} comparisons):\")\n    for i, result in enumerate(stability_results):\n        print(f\"  Comparison {i+1} (Window {i+1} vs {i+2}): SHD = {result[\'comparison_metrics\'][\'shd\']}\")\n\n
